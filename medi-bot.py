import os
import streamlit as st

from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq

from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())  # load .env file

DB_FAISS_PATH = "vectorstore/db_faiss"

# --- Cache FAISS vector store ---
@st.cache_resource
def get_vectorstore():
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)
    return db

# --- Custom Prompt ---
CUSTOM_PROMPT_TEMPLATE = """
Use the pieces of information provided in the context to answer the user's question.
If you don‚Äôt know the answer, say you don't know‚Äîdon‚Äôt fabricate.
Context: {context}
Question: {question}
Start the answer directly. No small talk.
"""

def set_custom_prompt(custom_prompt_template):
    return PromptTemplate(template=custom_prompt_template, input_variables=["context", "question"])

# --- Load LLM (Groq) ---
def load_llm():
    return ChatGroq(
        model="llama3-70b-8192",    
        temperature=0.0,
        api_key=os.environ["GROQ_API_KEY"]  # from .env
    )

# --- Streamlit App ---
def main():
    st.title("ü©∫ MediBot - Ask your Medical PDF")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat history
    for message in st.session_state.messages:
        st.chat_message(message["role"]).markdown(message["content"])

    # User input
    prompt = st.chat_input("Enter your medical question here...")

    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        try:
            vectorstore = get_vectorstore()
            if vectorstore is None:
                st.error("‚ùå Failed to load the vector store.")
                return

            qa_chain = RetrievalQA.from_chain_type(
                llm=load_llm(),
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
                return_source_documents=True,
                chain_type_kwargs={"prompt": set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)},
            )

            response = qa_chain.invoke({"query": prompt})

            result = response["result"]
            source_documents = response["source_documents"]

            st.chat_message("assistant").markdown(result)
            st.session_state.messages.append({"role": "assistant", "content": result})

            with st.expander("üìÑ Source Documents"):
                for i, doc in enumerate(source_documents):
                    st.markdown(f"**Source {i+1}:**")
                    st.write(doc.page_content[:500] + "...")  # show preview

        except Exception as e:
            st.error(f"‚ö†Ô∏è Error: {str(e)}")

if __name__ == "__main__":
    main()
